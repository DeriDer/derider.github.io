<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">

    <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>强化学习基本概念 | derider.github.io</title>
<meta name="generator" content="Jekyll v3.10.0" />
<meta property="og:title" content="强化学习基本概念" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="强化学习 (RL) 是一种机器学习方法，其中 代理 通过与环境交互来学习做出决策，以最大化累积奖励. 代理通过反复试验来学习，调整其策略以适应环境，而无需明确的指示. 强化学习的关键概念： Agent（代理） 强化学习中的学习者和决策者，通过在环境中采取行动来学习. Environment（环境） 代理与之交互的外部系统，提供状态信息并返回奖励或惩罚作为反馈. State（状态） 环境的当前情况，代理用来做出决策. Action（行动） 代理可以在特定状态下做出的选择. Reward（奖励） 代理在某个状态下采取行动后获得的立即反馈. Policy（策略） 代理用来决定基于状态采取什么行动的一组规则. Value Function（价值函数） 估计在策略下从特定状态开始预期的长期奖励. 强化学习算法： Q-Learning 一种无模型的算法，可学习状态-动作空间中动作的价值. Deep Q-Network (DQN) 使用深度神经网络处理大型状态空间的 Q-Learning 的扩展. Policy Gradient Methods（策略梯度方法） 通过使用梯度上升调整策略参数来直接优化策略. Actor-Critic Methods（演员-评论家方法） 结合了基于价值和基于策略的方法，其中演员更新策略，评论家评估行动. 强化学习的简单步骤： 定义环境 指定状态、操作、转换规则和奖励. 初始化策略和价值函数 建立决策和价值评估的初始策略. 观察初始状态 收集有关环境初始条件的信息. 选择一个动作 根据当前策略决定一个动作. 观察结果 接收来自环境的新状态和奖励形式的反馈. 更新策略 根据收到的反馈调整决策策略和价值评估. example：flappy bird" />
<meta property="og:description" content="强化学习 (RL) 是一种机器学习方法，其中 代理 通过与环境交互来学习做出决策，以最大化累积奖励. 代理通过反复试验来学习，调整其策略以适应环境，而无需明确的指示. 强化学习的关键概念： Agent（代理） 强化学习中的学习者和决策者，通过在环境中采取行动来学习. Environment（环境） 代理与之交互的外部系统，提供状态信息并返回奖励或惩罚作为反馈. State（状态） 环境的当前情况，代理用来做出决策. Action（行动） 代理可以在特定状态下做出的选择. Reward（奖励） 代理在某个状态下采取行动后获得的立即反馈. Policy（策略） 代理用来决定基于状态采取什么行动的一组规则. Value Function（价值函数） 估计在策略下从特定状态开始预期的长期奖励. 强化学习算法： Q-Learning 一种无模型的算法，可学习状态-动作空间中动作的价值. Deep Q-Network (DQN) 使用深度神经网络处理大型状态空间的 Q-Learning 的扩展. Policy Gradient Methods（策略梯度方法） 通过使用梯度上升调整策略参数来直接优化策略. Actor-Critic Methods（演员-评论家方法） 结合了基于价值和基于策略的方法，其中演员更新策略，评论家评估行动. 强化学习的简单步骤： 定义环境 指定状态、操作、转换规则和奖励. 初始化策略和价值函数 建立决策和价值评估的初始策略. 观察初始状态 收集有关环境初始条件的信息. 选择一个动作 根据当前策略决定一个动作. 观察结果 接收来自环境的新状态和奖励形式的反馈. 更新策略 根据收到的反馈调整决策策略和价值评估. example：flappy bird" />
<meta property="og:site_name" content="derider.github.io" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-02-16T00:00:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="强化学习基本概念" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-02-16T00:00:00+00:00","datePublished":"2025-02-16T00:00:00+00:00","description":"强化学习 (RL) 是一种机器学习方法，其中 代理 通过与环境交互来学习做出决策，以最大化累积奖励. 代理通过反复试验来学习，调整其策略以适应环境，而无需明确的指示. 强化学习的关键概念： Agent（代理） 强化学习中的学习者和决策者，通过在环境中采取行动来学习. Environment（环境） 代理与之交互的外部系统，提供状态信息并返回奖励或惩罚作为反馈. State（状态） 环境的当前情况，代理用来做出决策. Action（行动） 代理可以在特定状态下做出的选择. Reward（奖励） 代理在某个状态下采取行动后获得的立即反馈. Policy（策略） 代理用来决定基于状态采取什么行动的一组规则. Value Function（价值函数） 估计在策略下从特定状态开始预期的长期奖励. 强化学习算法： Q-Learning 一种无模型的算法，可学习状态-动作空间中动作的价值. Deep Q-Network (DQN) 使用深度神经网络处理大型状态空间的 Q-Learning 的扩展. Policy Gradient Methods（策略梯度方法） 通过使用梯度上升调整策略参数来直接优化策略. Actor-Critic Methods（演员-评论家方法） 结合了基于价值和基于策略的方法，其中演员更新策略，评论家评估行动. 强化学习的简单步骤： 定义环境 指定状态、操作、转换规则和奖励. 初始化策略和价值函数 建立决策和价值评估的初始策略. 观察初始状态 收集有关环境初始条件的信息. 选择一个动作 根据当前策略决定一个动作. 观察结果 接收来自环境的新状态和奖励形式的反馈. 更新策略 根据收到的反馈调整决策策略和价值评估. example：flappy bird","headline":"强化学习基本概念","mainEntityOfPage":{"@type":"WebPage","@id":"/2025/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.html"},"url":"/2025/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.html"}</script>
<!-- End Jekyll SEO tag -->

    <link rel="preconnect" href="https://fonts.gstatic.com">
    <link rel="preload" href="https://fonts.googleapis.com/css?family=Open+Sans:400,700&display=swap" as="style" type="text/css" crossorigin>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <link rel="stylesheet" href="/assets/css/style.css?v=46997005d295b7a48f7bbd742028427b1c5bcfae">
    <link rel="stylesheet" href="/assets/css/custom.css?v=46997005d295b7a48f7bbd742028427b1c5bcfae">
    <!-- start custom head snippets, customize with your own _includes/head-custom.html file -->

<!-- Setup Google Analytics -->



<!-- You can set your favicon here -->
<!-- link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" -->

<!-- end custom head snippets -->


    <style>
      nav a {
        text-decoration: none;
        color: #007bff;
        position: relative;
        transition: color 0.3s ease, transform 0.2s;
      }

      nav a::after {
        content: "";
        position: absolute;
        left: 0;
        bottom: -2px;
        width: 0;
        height: 2px;
        background-color: rgba(0, 0, 0, 0.3);
        transition: width 0.3s ease;
      }

      nav a:hover {
        color: #0056b3;
        transform: scale(1.3);
      }

      nav a:hover::after {
        width: 100%;
      }

      main {
        display: flex;
        flex-direction: column;
        align-items: center;
        justify-content: center;
        padding: 20px;
        min-height: 80vh;
        background-color: #1e1e1e;
        /* text-align: center; */
      }

      main h1 {
        font-size: 2.5rem;
        margin-bottom: 10px;
      }

      main p {
        color: #555;
        margin-bottom: 30px;
      }

      article {
        max-width: 800px;
        line-height: 1.8;
        text-align: left;
        
      }
    </style>
  </head>
  <body>
    <!-- Header 保持与 Default 一致 -->
    <header class="page-header" role="banner" style="display: flex; align-items: center; justify-content: space-between; padding: 10px 20px; background-color: #f8f9fa;">
      <h1 class="project-name">乔布立斯Blog</h1>
      <nav style="display: flex; gap: 15px; margin-left: auto;">
        <a href="/" style="margin-right: 15px; text-decoration: none; color: #ffffff;">Home</a>
        <a href="/about/" style="margin-right: 15px; text-decoration: none; color: #ffffff;">About</a>
        <a href="/archive/" style="text-decoration: none; color: #ffffff;">Archive</a>
      </nav>
      <h2 class="project-tagline"></h2>
    </header>

    <!-- Main Content -->
    <main>
      <!-- Post Title -->
      <h1>强化学习基本概念</h1>

      <!-- Post Metadata -->
      <p>
        Posted by DeriDer on 
        <time datetime="2025-02-16">February 16, 2025</time>
      </p>

      <!-- Post Content -->
      <article>
        <p>强化学习 (RL) 是一种机器学习方法，其中 <em>代理</em> 通过与环境交互来学习做出决策，以最大化累积奖励. 代理通过反复试验来学习，调整其策略以适应环境，而无需明确的指示.</p>

<p><strong>强化学习的关键概念：</strong></p>

<ul>
  <li><strong>Agent（代理）</strong> 强化学习中的学习者和决策者，通过在环境中采取行动来学习.</li>
  <li><strong>Environment（环境）</strong> 代理与之交互的外部系统，提供状态信息并返回奖励或惩罚作为反馈.</li>
  <li><strong>State（状态）</strong> 环境的当前情况，代理用来做出决策.</li>
  <li><strong>Action（行动）</strong> 代理可以在特定状态下做出的选择.</li>
  <li><strong>Reward（奖励）</strong> 代理在某个状态下采取行动后获得的立即反馈.</li>
  <li><strong>Policy（策略）</strong> 代理用来决定基于状态采取什么行动的一组规则.</li>
  <li><strong>Value Function（价值函数）</strong> 估计在策略下从特定状态开始预期的长期奖励.</li>
</ul>

<p><strong>强化学习算法：</strong></p>

<ul>
  <li><strong>Q-Learning</strong> 一种无模型的算法，可学习状态-动作空间中动作的价值.</li>
  <li><strong>Deep Q-Network (DQN)</strong> 使用深度神经网络处理大型状态空间的 Q-Learning 的扩展.</li>
  <li><strong>Policy Gradient Methods（策略梯度方法）</strong> 通过使用梯度上升调整策略参数来直接优化策略.</li>
  <li><strong>Actor-Critic Methods（演员-评论家方法）</strong> 结合了基于价值和基于策略的方法，其中演员更新策略，评论家评估行动.</li>
</ul>

<p><strong>强化学习的简单步骤：</strong></p>

<ol>
  <li><strong>定义环境</strong> 指定状态、操作、转换规则和奖励.</li>
  <li><strong>初始化策略和价值函数</strong> 建立决策和价值评估的初始策略.</li>
  <li><strong>观察初始状态</strong> 收集有关环境初始条件的信息.</li>
  <li><strong>选择一个动作</strong> 根据当前策略决定一个动作.</li>
  <li><strong>观察结果</strong> 接收来自环境的新状态和奖励形式的反馈.</li>
  <li><strong>更新策略</strong> 根据收到的反馈调整决策策略和价值评估.</li>
</ol>

<p><strong>example：flappy bird</strong></p>

      </article>

      <!-- Previous/Next Navigation -->
      <nav style="margin-top: 40px;">
        
          <a href="/2025/02/11/mnq%E5%A4%8D%E7%9B%98.html" style="margin-right: 20px;">&larr; Previous: 20250211MNQ复盘</a>
        
        
          <a href="/2025/02/19/mnq%E5%A4%8D%E7%9B%98.html">Next: 20250219MNQ复盘 &rarr;</a>
        
      </nav>
    </main>

    <!-- Footer -->
    <footer class="site-footer" style="text-align: center; padding: 10px; background-color: #f8f9fa;">
      
        <span class="site-footer-owner"><a href="https://github.com/DeriDer/derider.github.io">derider.github.io</a> is maintained by <a href="https://github.com/DeriDer">DeriDer</a>.</span>
      
      <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a>.</span>
    </footer>
  </body>
</html>
